Title: Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization

Abstract: In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statisticallearningalgorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of\mathcal{O}(1/\sqrt{n})\mathcal{O}(1/\sqrt{n}), wherennis the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data "memorization" raised in those works, and which asserts that there arelearningproblem instances for which anylearningalgorithm that has good prediction there exist distributions under which the algorithm must "memorize" a big fraction of the training dataset. We show that for everylearningalgorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization.â–³ Less
