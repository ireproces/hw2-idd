Title: An Information-Theoretic Analysis of Out-of-Distribution Generalization in Meta-with Applications to Meta-RL

Abstract: In this work, we study out-of-distribution generalization in meta-learningfrom an information-theoretic perspective. We focus on two scenarios: (i) when the testing environment mismatches the training environment, and (ii) when the training environment is broader than the testing environment. The first corresponds to the standard distribution mismatch setting, while the second reflects a broad-to-narrow training scenario. We further formalize the generalization problem in meta-reinforcementlearningand establish corresponding generalization bounds. Finally, we analyze the generalization performance of a gradient-based meta-reinforcementlearningalgorithm.â–³ Less
