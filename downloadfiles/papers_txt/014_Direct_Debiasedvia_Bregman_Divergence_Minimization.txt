Title: Direct Debiasedvia Bregman Divergence Minimization

Abstract: We develop a direct debiasedmachinelearningframework comprising Neyman targeted estimation and generalized Riesz regression. Our framework unifies Riesz regression for automatic debiasedmachinelearning, covariate balancing, targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In many problems involving causal effects or structural models, the parameters of interest depend on regression functions. Plugging regression functions estimated bymachinelearningmethods into the identifying equations can yield poor performance because of first-stage bias. To reduce such bias, debiasedmachinelearningemploys Neyman orthogonal estimating equations. Debiasedmachinelearningtypically requires estimation of the Riesz representer and the regression function. For this problem, we develop a direct debiasedmachinelearningframework with an end-to-end algorithm. We formulate estimation of the nuisance parameters, the regression function and the Riesz representer, as minimizing the discrepancy between Neyman orthogonal scores computed with known and unknown nuisance parameters, which we refer to as Neyman targeted estimation. Neyman targeted estimation includes Riesz representer estimation, and we measure discrepancies using the Bregman divergence. The Bregman divergence encompasses various loss functions as special cases, where the squared loss yields Riesz regression and the Kullback-Leibler divergence yields entropy balancing. We refer to this Riesz representer estimation as generalized Riesz regression. Neyman targeted estimation also yields TMLE as a special case for regression function estimation. Furthermore, for specific pairs of models and Riesz representer estimation methods, we can automatically obtain the covariate balancing property without explicitly solving the covariate balancing objective.â–³ Less
