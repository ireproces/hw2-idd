<html lang="en"><head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer="" src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css">
<script type="text/x-mathjax-config;executed=true">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src="//static.arxiv.org/MathJax-2.7.3/MathJax.js"></script><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css">
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css">
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g=" crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.3') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.3') format('woff'), url('https://static.arxiv.org/MathJax-2.7.3/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.3') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: normal; text-align: left; color: black; padding: 1px 3px; border: }
</style></head>
  <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo"></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;">
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms">
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1–50 of 266,545 results for all: <span class="mathjax">Machine Learning</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs" aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Machine Learning">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option selected="" value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked="" id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Machine+Learning&amp;terms-0-field=all&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option selected="" value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Machine Learning">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked="" id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected="" value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected="" value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="" class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50" class="pagination-next">Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0" class="pagination-link is-current" aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50" class="pagination-link " aria-label="Page 2" aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100" class="pagination-link " aria-label="Page 3" aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150" class="pagination-link " aria-label="Page 4" aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200" class="pagination-link " aria-label="Page 5" aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">…</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23606">arXiv:2510.23606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23606">pdf</a>, <a href="https://arxiv.org/ps/2510.23606">ps</a>, <a href="https://arxiv.org/format/2510.23606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Variational Masked Diffusion Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yichi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schwing%2C+A">Alex Schwing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Z">Zhizhen Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23606v1-abstract-short" style="display: inline;">
        …a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully <span class="search-hit mathjax">learns</span> dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23606v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23606v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23606v1-abstract-full" style="display: none;">
        Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully <span class="search-hit mathjax">learns</span> dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where <span class="search-hit mathjax">learning</span> of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23606v1-abstract-full').style.display = 'none'; document.getElementById('2510.23606v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Page: https://riccizz.github.io/VMD</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23605">arXiv:2510.23605</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23605">pdf</a>, <a href="https://arxiv.org/ps/2510.23605">ps</a>, <a href="https://arxiv.org/format/2510.23605">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+S">Shuhong Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mirzaei%2C+A">Ashkan Mirzaei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gilitschenski%2C+I">Igor Gilitschenski</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23605v1-abstract-short" style="display: inline;">
        Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23605v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23605v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23605v1-abstract-full" style="display: none;">
        Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23605v1-abstract-full').style.display = 'none'; document.getElementById('2510.23605v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2025, 38 pages, 22 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23590">arXiv:2510.23590</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23590">pdf</a>, <a href="https://arxiv.org/ps/2510.23590">ps</a>, <a href="https://arxiv.org/format/2510.23590">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lightweight Robust Direct Preference Optimization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+C+W">Cheol Woo Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Verma%2C+S">Shresth Verma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tec%2C+M">Mauricio Tec</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tambe%2C+M">Milind Tambe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23590v1-abstract-short" style="display: inline;">
        Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23590v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23590v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23590v1-abstract-full" style="display: none;">
        Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods often suffer from excessive conservatism and high computational cost. We propose DPO-PRO (DPO with Preference Robustness), a robust fine-tuning algorithm based on DPO which accounts for uncertainty in the preference distribution through a lightweight DRO formulation. Unlike prior DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences, avoiding unnecessary conservatism and incurring negligible computational overhead. We further show that DPO-PRO is equivalent to a regularized DPO objective that penalizes model overconfidence under weak preference signals. We evaluate DPO-PRO on standard alignment benchmarks and a real-world public health task. Experimental results show that our method consistently improves robustness to noisy preference signals compared to existing DPO variants.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23590v1-abstract-full').style.display = 'none'; document.getElementById('2510.23590v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23588">arXiv:2510.23588</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23588">pdf</a>, <a href="https://arxiv.org/ps/2510.23588">ps</a>, <a href="https://arxiv.org/format/2510.23588">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FARMER: Flow AutoRegressive Transformer over Pixels
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+G">Guangting Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Q">Qinyu Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+T">Tao Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+F">Fei Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Z">Zhijie Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jie Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+J">Jiajun Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yanyong Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+R">Rui Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23588v1-abstract-short" style="display: inline;">
        Directly modeling the explicit likelihood of the raw data distribution is key topic in the <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spa…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23588v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23588v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23588v1-abstract-full" style="display: none;">
        Directly modeling the explicit likelihood of the raw data distribution is key topic in the <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23588v1-abstract-full').style.display = 'none'; document.getElementById('2510.23588v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Bytedance Seed Technical Report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23585">arXiv:2510.23585</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23585">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ramos%2C+L">Luis Ramos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Calvo%2C+H">Hiram Calvo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kolesnikova%2C+O">Olga Kolesnikova</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23585v1-abstract-short" style="display: inline;">
        …promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional <span class="search-hit mathjax">machine</span>…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23585v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23585v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23585v1-abstract-full" style="display: none;">
        The identification of hope speech has become a promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> models and fine-tuned transformers for a previously split hope speech dataset as train, development and test set. On development test, a linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM with RBF kernel reached 0.77, and Naïve Bayes hit 0.75. Transformer models delivered better results, the best model achieved weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80 accuracy. These results suggest that while optimally configured traditional <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> models remain agile, transformer architectures detect some subtle semantics of hope to achieve higher precision and recall in hope speech detection, suggesting that larges transformers and LLMs could perform better in small datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23585v1-abstract-full').style.display = 'none'; document.getElementById('2510.23585v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23581">arXiv:2510.23581</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23581">pdf</a>, <a href="https://arxiv.org/ps/2510.23581">ps</a>, <a href="https://arxiv.org/format/2510.23581">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Seo%2C+J">Junyoung Seo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mira%2C+R">Rodrigo Mira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Haliassos%2C+A">Alexandros Haliassos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bounareli%2C+S">Stella Bounareli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Honglie Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tran%2C+L">Linh Tran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Seungryong Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Landgraf%2C+Z">Zoe Landgraf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+J">Jie Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23581v1-abstract-short" style="display: inline;">
        Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Looka…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23581v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23581v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23581v1-abstract-full" style="display: none;">
        Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23581v1-abstract-full').style.display = 'none'; document.getElementById('2510.23581v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://lookahead-anchoring.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23577">arXiv:2510.23577</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23577">pdf</a>, <a href="https://arxiv.org/ps/2510.23577">ps</a>, <a href="https://arxiv.org/format/2510.23577">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Z">Zhongyi Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianqiu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Z">Zhenghao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+S">Shuhan Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+W">Weifeng Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+C">Chul-Ho Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuo%2C+W">Weipeng Zhuo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23577v1-abstract-short" style="display: inline;">
        …of past interactions for a pair of nodes that interact intermittently for their link prediction. Existing methods, however, do not consider such heterogeneity in their <span class="search-hit mathjax">learning</span> process, and thus their <span class="search-hit mathjax">learned</span> temporal node embeddings are less effective, especially when predicting the links for infrequently interacting…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23577v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23577v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23577v1-abstract-full" style="display: none;">
        Temporal graph link prediction aims to predict future interactions between nodes in a graph based on their historical interactions, which are encoded in node embeddings. We observe that heterogeneity naturally appears in temporal interactions, e.g., a few node pairs can make most interaction events, and interaction events happen at varying intervals. This leads to the problems of ineffective temporal information encoding and forgetting of past interactions for a pair of nodes that interact intermittently for their link prediction. Existing methods, however, do not consider such heterogeneity in their <span class="search-hit mathjax">learning</span> process, and thus their <span class="search-hit mathjax">learned</span> temporal node embeddings are less effective, especially when predicting the links for infrequently interacting node pairs. To cope with the heterogeneity, we propose a novel framework called TAMI, which contains two effective components, namely log time encoding function (LTE) and link history aggregation (LHA). LTE better encodes the temporal information through transforming interaction intervals into more balanced ones, and LHA prevents the historical interactions for each target node pair from being forgotten. State-of-the-art temporal graph neural networks can be seamlessly and readily integrated into TAMI to improve their effectiveness. Experiment results on 13 classic datasets and three newest temporal graph benchmark (TGB) datasets show that TAMI consistently improves the link prediction performance of the underlying models in both transductive and inductive settings. Our code is available at https://github.com/Alleinx/TAMI_temporal_graph.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23577v1-abstract-full').style.display = 'none'; document.getElementById('2510.23577v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NeurIPS 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23571">arXiv:2510.23571</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23571">pdf</a>, <a href="https://arxiv.org/ps/2510.23571">ps</a>, <a href="https://arxiv.org/format/2510.23571">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RobotArena <span class="MathJax_Preview">\infty</span><script type="math/tex" id="MathJax-Element-1">\infty</script>: Scalable Robot Benchmarking via Real-to-Sim Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jangir%2C+Y">Yash Jangir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yidi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yamazaki%2C+K">Kashu Yamazaki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Chenyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+K">Kuan-Hsun Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+T">Tsung-Wei Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+L">Lei Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bisk%2C+Y">Yonatan Bisk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fragkiadaki%2C+K">Katerina Fragkiadaki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23571v1-abstract-short" style="display: inline;">
        The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies w…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23571v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23571v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23571v1-abstract-full" style="display: none;">
        The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining "success" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23571v1-abstract-full').style.display = 'none'; document.getElementById('2510.23571v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Website: https://robotarenainf.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23564">arXiv:2510.23564</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23564">pdf</a>, <a href="https://arxiv.org/ps/2510.23564">ps</a>, <a href="https://arxiv.org/format/2510.23564">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ReCode: Unify Plan and Action for Universal Granularity Control
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Z">Zhaoyang Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jiayi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+H">Huixue Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yufan Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yifan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+M">Mingyi Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+J">Jinyu Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yizhang Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+L">Lingxiao Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yingchao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Y">Yuyu Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+B">Bang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chenglin Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23564v1-abstract-short" style="display: inline;">
        …the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to <span class="search-hit mathjax">learn</span> hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data eff…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23564v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23564v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23564v1-abstract-full" style="display: none;">
        Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to <span class="search-hit mathjax">learn</span> hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23564v1-abstract-full').style.display = 'none'; document.getElementById('2510.23564v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23557">arXiv:2510.23557</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23557">pdf</a>, <a href="https://arxiv.org/ps/2510.23557">ps</a>, <a href="https://arxiv.org/format/2510.23557">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Minimizing Human Intervention in Online Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9veillard%2C+W">William Réveillard</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saketos%2C+V">Vasileios Saketos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Proutiere%2C+A">Alexandre Proutiere</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Combes%2C+R">Richard Combes</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23557v1-abstract-short" style="display: inline;">
        …, one can <span class="search-hit mathjax">learn</span> the geometry of the class regions: in this regime, we propose the Conservative Hull-based Classifier (CHC), which maintains convex hulls of expert-labeled queries and calls the expert as soon as a query lands outside all known hulls. CHC attains <span class="MathJax_Preview">\mathcal{O}(\log^d T)</span><script type="math/tex" id="MathJax-Element-2">\mathcal{O}(\log^d T)</script> regret in <span class="MathJax_Preview">T</span><script type="math/tex" id="MathJax-Element-3">T</script> and is minimax optimal for <span class="MathJax_Preview">d=1</span><script type="math/tex" id="MathJax-Element-4">d=1</script>. Otherwise, the geometry ca…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23557v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23557v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23557v1-abstract-full" style="display: none;">
        We introduce and study an online problem arising in question answering systems. In this problem, an agent must sequentially classify user-submitted queries represented by <span class="MathJax_Preview">d</span><script type="math/tex">d</script>-dimensional embeddings drawn i.i.d. from an unknown distribution. The agent may consult a costly human expert for the correct label, or guess on her own without receiving feedback. The goal is to minimize regret against an oracle with free expert access. When the time horizon <span class="MathJax_Preview">T</span><script type="math/tex">T</script> is at least exponential in the embedding dimension <span class="MathJax_Preview">d</span><script type="math/tex">d</script>, one can <span class="search-hit mathjax">learn</span> the geometry of the class regions: in this regime, we propose the Conservative Hull-based Classifier (CHC), which maintains convex hulls of expert-labeled queries and calls the expert as soon as a query lands outside all known hulls. CHC attains <span class="MathJax_Preview">\mathcal{O}(\log^d T)</span><script type="math/tex">\mathcal{O}(\log^d T)</script> regret in <span class="MathJax_Preview">T</span><script type="math/tex">T</script> and is minimax optimal for <span class="MathJax_Preview">d=1</span><script type="math/tex">d=1</script>. Otherwise, the geometry cannot be reliably <span class="search-hit mathjax">learned</span> without additional distributional assumptions. We show that when the queries are drawn from a subgaussian mixture, for <span class="MathJax_Preview">T \le e^d</span><script type="math/tex">T \le e^d</script>, a Center-based Classifier (CC) achieves regret proportional to <span class="MathJax_Preview">N\log{N}</span><script type="math/tex">N\log{N}</script> where <span class="MathJax_Preview">N</span><script type="math/tex">N</script> is the number of labels. To bridge these regimes, we introduce the Generalized Hull-based Classifier (GHC), a practical extension of CHC that allows for more aggressive guessing via a tunable threshold parameter. Our approach is validated with experiments, notably on real-world question-answering datasets using embeddings derived from state-of-the-art large language models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23557v1-abstract-full').style.display = 'none'; document.getElementById('2510.23557v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">49 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23554">arXiv:2510.23554</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23554">pdf</a>, <a href="https://arxiv.org/ps/2510.23554">ps</a>, <a href="https://arxiv.org/format/2510.23554">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A U-Net and Transformer Pipeline for Multilingual Image Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sahay%2C+S">Siddharth Sahay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Agarwal%2C+R">Radhika Agarwal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23554v1-abstract-short" style="display: inline;">
        …pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural <span class="search-hit mathjax">Machine</span> Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regio…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23554v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23554v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23554v1-abstract-full" style="display: none;">
        This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural <span class="search-hit mathjax">Machine</span> Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23554v1-abstract-full').style.display = 'none'; document.getElementById('2510.23554v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 3 figures, 5 tables, and 2 algorithms. Prepared in IEEE double-column format</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23552">arXiv:2510.23552</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23552">pdf</a>, <a href="https://arxiv.org/ps/2510.23552">ps</a>, <a href="https://arxiv.org/format/2510.23552">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Probability">math.PR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generalized Kantorovich-Rubinstein Duality beyond Hausdorff and Kantorovich
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wild%2C+P">Paul Wild</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schr%C3%B6der%2C+L">Lutz Schröder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Messing%2C+K">Karla Messing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=K%C3%B6nig%2C+B">Barbara König</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Forster%2C+J">Jonas Forster</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23552v1-abstract-short" style="display: inline;">
        …the generalized Kantorovich-Rubinstein duality in this sense for two important cases: The Lévy-Prokhorov distance on distributions, which finds wide-spread applications in <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> due to its favourable stability properties, and the standard metric on convex sets of distributions that arises by combining the Ha…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23552v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23552v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23552v1-abstract-full" style="display: none;">
        The classical Kantorovich-Rubinstein duality guarantees coincidence between metrics on the space of probability distributions defined on the one hand via transport plans (couplings) and on the other hand via price functions. Both constructions have been lifted to the level of generality of set functors, with the coupling-based construction referred to as the Wasserstein lifting, and the price-function-based construction as the Kantorovich lifting, both based on a choice of quantitative modalities for the given functor. It is known that every Wasserstein lifting can be expressed as a Kantorovich lifting; however, the latter in general needs to use additional modalities. We give an example showing that this cannot be avoided in general. We refer to cases in which the same modalities can be used as satisfying the generalized Kantorovich-Rubinstein duality. We establish the generalized Kantorovich-Rubinstein duality in this sense for two important cases: The Lévy-Prokhorov distance on distributions, which finds wide-spread applications in <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> due to its favourable stability properties, and the standard metric on convex sets of distributions that arises by combining the Hausdorff and Wasserstein distances.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23552v1-abstract-full').style.display = 'none'; document.getElementById('2510.23552v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          03B45; 03B52; 68Q85
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.4.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23535">arXiv:2510.23535</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23535">pdf</a>, <a href="https://arxiv.org/ps/2510.23535">ps</a>, <a href="https://arxiv.org/format/2510.23535">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sequential Multi-Agent Dynamic Algorithm Configuration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+C">Chen Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+K">Ke Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+L">Lei Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yaoyuan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+S">Sheng Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+C">Chao Qian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23535v1-abstract-short" style="display: inline;">
        Dynamic algorithm configuration (DAC) is a recent trend in automated <span class="search-hit mathjax">machine</span>…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23535v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23535v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23535v1-abstract-full" style="display: none;">
        Dynamic algorithm configuration (DAC) is a recent trend in automated <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>, which can dynamically adjust the algorithm's configuration during the execution process and relieve users from tedious trial-and-error tuning tasks. Recently, multi-agent reinforcement <span class="search-hit mathjax">learning</span> (MARL) approaches have improved the configuration of multiple heterogeneous hyperparameters, making various parameter configurations for complex algorithms possible. However, many complex algorithms have inherent inter-dependencies among multiple parameters (e.g., determining the operator type first and then the operator's parameter), which are, however, not considered in previous approaches, thus leading to sub-optimal results. In this paper, we propose the sequential multi-agent DAC (Seq-MADAC) framework to address this issue by considering the inherent inter-dependencies of multiple parameters. Specifically, we propose a sequential advantage decomposition network, which can leverage action-order information through sequential advantage decomposition. Experiments from synthetic functions to the configuration of multi-objective optimization algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art MARL methods and show strong generalization across problem classes. Seq-MADAC establishes a new paradigm for the widespread dependency-aware automated algorithm configuration. Our code is available at https://github.com/lamda-bbo/seq-madac.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23535v1-abstract-full').style.display = 'none'; document.getElementById('2510.23535v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23534">arXiv:2510.23534</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23534">pdf</a>, <a href="https://arxiv.org/ps/2510.23534">ps</a>, <a href="https://arxiv.org/format/2510.23534">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Econometrics">econ.EM</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistics Theory">math.ST</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Direct Debiased <span class="search-hit mathjax">Machine</span> <span class="search-hit mathjax">Learning</span> via Bregman Divergence Minimization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kato%2C+M">Masahiro Kato</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23534v1-abstract-short" style="display: inline;">
        We develop a direct debiased <span class="search-hit mathjax">machine</span>…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23534v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23534v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23534v1-abstract-full" style="display: none;">
        We develop a direct debiased <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> framework comprising Neyman targeted estimation and generalized Riesz regression. Our framework unifies Riesz regression for automatic debiased <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>, covariate balancing, targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In many problems involving causal effects or structural models, the parameters of interest depend on regression functions. Plugging regression functions estimated by <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> methods into the identifying equations can yield poor performance because of first-stage bias. To reduce such bias, debiased <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> employs Neyman orthogonal estimating equations. Debiased <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> typically requires estimation of the Riesz representer and the regression function. For this problem, we develop a direct debiased <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> framework with an end-to-end algorithm. We formulate estimation of the nuisance parameters, the regression function and the Riesz representer, as minimizing the discrepancy between Neyman orthogonal scores computed with known and unknown nuisance parameters, which we refer to as Neyman targeted estimation. Neyman targeted estimation includes Riesz representer estimation, and we measure discrepancies using the Bregman divergence. The Bregman divergence encompasses various loss functions as special cases, where the squared loss yields Riesz regression and the Kullback-Leibler divergence yields entropy balancing. We refer to this Riesz representer estimation as generalized Riesz regression. Neyman targeted estimation also yields TMLE as a special case for regression function estimation. Furthermore, for specific pairs of models and Riesz representer estimation methods, we can automatically obtain the covariate balancing property without explicitly solving the covariate balancing objective.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23534v1-abstract-full').style.display = 'none'; document.getElementById('2510.23534v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23532">arXiv:2510.23532</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23532">pdf</a>, <a href="https://arxiv.org/ps/2510.23532">ps</a>, <a href="https://arxiv.org/format/2510.23532">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Das%2C+A">Anirban Das</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khalid%2C+I">Irtaza Khalid</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pe%C3%B1aloza%2C+R">Rafael Peñaloza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schockaert%2C+S">Steven Schockaert</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23532v1-abstract-short" style="display: inline;">
        Designing models that can <span class="search-hit mathjax">learn</span> to reason in a systematic way is an important and long-standing challenge. In recent years, a wide range of solutions have been proposed for the specific case of systematic relational reasoning, including Neuro-Symbolic approaches, variants of the Transformer architecture, and specialised Graph Neural Networks. However, existi…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23532v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23532v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23532v1-abstract-full" style="display: none;">
        Designing models that can <span class="search-hit mathjax">learn</span> to reason in a systematic way is an important and long-standing challenge. In recent years, a wide range of solutions have been proposed for the specific case of systematic relational reasoning, including Neuro-Symbolic approaches, variants of the Transformer architecture, and specialised Graph Neural Networks. However, existing benchmarks for systematic relational reasoning focus on an overly simplified setting, based on the assumption that reasoning can be reduced to composing relational paths. In fact, this assumption is hard-baked into the architecture of several recent models, leading to approaches that can perform well on existing benchmarks but are difficult to generalise to other settings. To support further progress in the field of systematic relational reasoning with neural networks, we introduce NoRA, a new benchmark which adds several levels of difficulty and requires models to go beyond path-based reasoning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23532v1-abstract-full').style.display = 'none'; document.getElementById('2510.23532v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted at NeurIPS 2025 D&amp;B track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23530">arXiv:2510.23530</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23530">pdf</a>, <a href="https://arxiv.org/ps/2510.23530">ps</a>, <a href="https://arxiv.org/format/2510.23530">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Learning</span> Linearity in Audio Consistency Autoencoders via Implicit Regularization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Torres%2C+B">Bernardo Torres</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moussallam%2C+M">Manuel Moussallam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meseguer-Brocal%2C+G">Gabriel Meseguer-Brocal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23530v1-abstract-short" style="display: inline;">
        Audio autoencoders <span class="search-hit mathjax">learn</span> useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to s…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23530v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23530v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23530v1-abstract-full" style="display: none;">
        Audio autoencoders <span class="search-hit mathjax">learn</span> useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to scalar gain) and additivity (the decoder preserves addition) without altering the model's architecture or loss function. When trained with our method, the CAE exhibits linear behavior in both the encoder and decoder while preserving reconstruction fidelity. We test the practical utility of our <span class="search-hit mathjax">learned</span> space on music source composition and separation via simple latent arithmetic. This work presents a straightforward technique for constructing structured latent spaces, enabling more intuitive and efficient audio processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23530v1-abstract-full').style.display = 'none'; document.getElementById('2510.23530v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23528">arXiv:2510.23528</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23528">pdf</a>, <a href="https://arxiv.org/ps/2510.23528">ps</a>, <a href="https://arxiv.org/format/2510.23528">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tracing Distribution Shifts with Causal System Maps
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Leest%2C+J">Joran Leest</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gerostathopoulos%2C+I">Ilias Gerostathopoulos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lago%2C+P">Patricia Lago</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Raibulet%2C+C">Claudia Raibulet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23528v1-abstract-short" style="display: inline;">
        Monitoring <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> (ML) systems is hard, with standard practice focusing on detecting distribution shifts rather than their causes. Root-cause analysis often relies on manual tracing to determine whether a shift is caused by software faults, data-quality issues, or natural change. We propose ML System Maps --…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23528v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23528v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23528v1-abstract-full" style="display: none;">
        Monitoring <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> (ML) systems is hard, with standard practice focusing on detecting distribution shifts rather than their causes. Root-cause analysis often relies on manual tracing to determine whether a shift is caused by software faults, data-quality issues, or natural change. We propose ML System Maps -- causal maps that, through layered views, make explicit the propagation paths between the environment and the ML system's internals, enabling systematic attribution of distribution shifts. We outline the approach and a research agenda for its development and evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23528v1-abstract-full').style.display = 'none'; document.getElementById('2510.23528v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23524">arXiv:2510.23524</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23524">pdf</a>, <a href="https://arxiv.org/ps/2510.23524">ps</a>, <a href="https://arxiv.org/format/2510.23524">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and <span class="search-hit mathjax">Learning</span> Paradigms for Sustainable Intelligence
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Santosh%2C+K">KC Santosh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rizk%2C+R">Rodrigue Rizk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Longwei Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23524v1-abstract-short" style="display: inline;">
        …training paradigms, advocating for a shift toward human-inspired, sustainable AI solutions. We introduce a novel framework, Human AI (HAI), which emphasizes incremental <span class="search-hit mathjax">learning</span>, carbon-aware optimization, and human-in-the-loop collaboration to enhance adaptability, efficiency, and accountability. By drawing parallels with biological cognition and leveraging…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23524v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23524v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23524v1-abstract-full" style="display: none;">
        The rapid advancement of Artificial Intelligence (AI) has led to unprecedented computational demands, raising significant environmental and ethical concerns. This paper critiques the prevailing reliance on large-scale, static datasets and monolithic training paradigms, advocating for a shift toward human-inspired, sustainable AI solutions. We introduce a novel framework, Human AI (HAI), which emphasizes incremental <span class="search-hit mathjax">learning</span>, carbon-aware optimization, and human-in-the-loop collaboration to enhance adaptability, efficiency, and accountability. By drawing parallels with biological cognition and leveraging dynamic architectures, HAI seeks to balance performance with ecological responsibility. We detail the theoretical foundations, system design, and operational principles that enable AI to <span class="search-hit mathjax">learn</span> continuously and contextually while minimizing carbon footprints and human annotation costs. Our approach addresses pressing challenges in active <span class="search-hit mathjax">learning</span>, continual adaptation, and energy-efficient model deployment, offering a pathway toward responsible, human-centered artificial intelligence.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23524v1-abstract-full').style.display = 'none'; document.getElementById('2510.23524v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23507">arXiv:2510.23507</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23507">pdf</a>, <a href="https://arxiv.org/ps/2510.23507">ps</a>, <a href="https://arxiv.org/format/2510.23507">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off Perspective
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ghodsi%2C+S">Siamak Ghodsi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seyedi%2C+A">Amjad Seyedi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Quy%2C+T+L">Tai Le Quy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Karimi%2C+F">Fariba Karimi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ntoutsi%2C+E">Eirini Ntoutsi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23507v1-abstract-short" style="display: inline;">
        Fair graph clustering seeks partitions that respect network structure while maintaining proportional representation across sensitive groups, with applications spanning community detection, team formation, resource allocation, and social network analysis. Many existing approaches enforce rigid constraints or rely on multi-stage pipelines (e.g., spectral embedding followed by <span class="MathJax_Preview">k</span><script type="math/tex">k</script>-means), limiting tr…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23507v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23507v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23507v1-abstract-full" style="display: none;">
        Fair graph clustering seeks partitions that respect network structure while maintaining proportional representation across sensitive groups, with applications spanning community detection, team formation, resource allocation, and social network analysis. Many existing approaches enforce rigid constraints or rely on multi-stage pipelines (e.g., spectral embedding followed by <span class="MathJax_Preview">k</span><script type="math/tex">k</script>-means), limiting trade-off control, interpretability, and scalability. We introduce \emph{DFNMF}, an end-to-end deep nonnegative tri-factorization tailored to graphs that directly optimizes cluster assignments with a soft statistical-parity regularizer. A single parameter <span class="MathJax_Preview">λ</span><script type="math/tex">λ</script> tunes the fairness--utility balance, while nonnegativity yields parts-based factors and transparent soft memberships. The optimization uses sparse-friendly alternating updates and scales near-linearly with the number of edges. Across synthetic and real networks, DFNMF achieves substantially higher group balance at comparable modularity, often dominating state-of-the-art baselines on the Pareto front. The code is available at https://github.com/SiamakGhodsi/DFNMF.git.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23507v1-abstract-full').style.display = 'none'; document.getElementById('2510.23507v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE Big-Data 2025 main research track. The paper is 10 main pages and 4 pages of Appendix</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23503">arXiv:2510.23503</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23503">pdf</a>, <a href="https://arxiv.org/ps/2510.23503">ps</a>, <a href="https://arxiv.org/format/2510.23503">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Safaeipour%2C+F+Z">Fatemeh Zahra Safaeipour</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chakareski%2C+J">Jacob Chakareski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hashemi%2C+M">Morteza Hashemi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23503v1-abstract-short" style="display: inline;">
        Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely inference tasks while operating with limited on-board computing and energy resources. In this paper, we investigate the problem of collaborative inference in wireless edge networks, where energy-constrained edge devices aim to complete inference tasks within given deadlines. These tasks are carried out using neural networ…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23503v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23503v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23503v1-abstract-full" style="display: none;">
        Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely inference tasks while operating with limited on-board computing and energy resources. In this paper, we investigate the problem of collaborative inference in wireless edge networks, where energy-constrained edge devices aim to complete inference tasks within given deadlines. These tasks are carried out using neural networks, and the edge device seeks to optimize inference performance under energy and delay constraints. The inference process can be split between the edge device and an edge server, thereby achieving collaborative inference over wireless networks. We formulate an inference utility optimization problem subject to energy and delay constraints, and propose a novel solution called Bayes-Split-Edge, which leverages Bayesian optimization for collaborative split inference over wireless edge networks. Our solution jointly optimizes the transmission power and the neural network split point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition function that balances inference task utility, sample efficiency, and constraint violation penalties. We evaluate our approach using the VGG19 model on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world mMobile wireless channel datasets. Numerical results demonstrate that Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to standard Bayesian optimization and achieves near-linear convergence. It also outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and Proximal Policy Optimization (PPO), while matching exhaustive search performance under tight constraints. These results confirm that the proposed framework provides a sample-efficient solution requiring maximum 20 function evaluations and constraint-aware optimization for wireless split inference in edge computing systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23503v1-abstract-full').style.display = 'none'; document.getElementById('2510.23503v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23501">arXiv:2510.23501</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23501">pdf</a>, <a href="https://arxiv.org/ps/2510.23501">ps</a>, <a href="https://arxiv.org/format/2510.23501">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Deep Physics-Informed Kolmogorov-Arnold Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Rigas%2C+S">Spyros Rigas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anagnostopoulos%2C+F">Fotios Anagnostopoulos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Papachristou%2C+M">Michalis Papachristou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alexandridis%2C+G">Georgios Alexandridis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23501v1-abstract-short" style="display: inline;">
        Since their introduction, Kolmogorov-Arnold Networks (KANs) have been successfully applied across several domains, with physics-informed <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> (PIML) emerging as one of the areas where they have thrived. In the PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the standard due to thei…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23501v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23501v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23501v1-abstract-full" style="display: none;">
        Since their introduction, Kolmogorov-Arnold Networks (KANs) have been successfully applied across several domains, with physics-informed <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> (PIML) emerging as one of the areas where they have thrived. In the PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the standard due to their computational efficiency. However, like their multilayer perceptron-based counterparts, cPIKANs face significant challenges when scaled to depth, leading to training instabilities that limit their applicability to several PDE problems. To address this, we propose a basis-agnostic, Glorot-like initialization scheme that preserves activation variance and yields substantial improvements in stability and accuracy over the default initialization of cPIKANs. Inspired by the PirateNet architecture, we further introduce Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in deep cPIKANs where initialization alone is not sufficient. Through empirical tests and information bottleneck analysis, we show that RGA KANs successfully traverse all training phases, unlike baseline cPIKANs, which stagnate in the diffusion phase in specific PDE settings. Evaluations on seven standard forward PDE benchmarks under a fixed training pipeline with adaptive components demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and PirateNets - often by several orders of magnitude - while remaining stable in settings where the others diverge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23501v1-abstract-full').style.display = 'none'; document.getElementById('2510.23501v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">73 pages, 22 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23498">arXiv:2510.23498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23498">pdf</a>, <a href="https://arxiv.org/ps/2510.23498">ps</a>, <a href="https://arxiv.org/format/2510.23498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mixed Precision Training of Neural ODEs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Celledoni%2C+E">Elena Celledoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Owren%2C+B">Brynjulf Owren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ruthotto%2C+L">Lars Ruthotto</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+T+N">Tianjiao Nicole Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23498v1-abstract-short" style="display: inline;">
        Exploiting low-precision computations has become a standard strategy in deep <span class="search-hit mathjax">learning</span> to address the growing computational costs imposed by ever larger models and datasets. However, naively performing all computations in low precision can lead to roundoff errors and instabilities. Therefore, mixed precision training schemes usually store the weights in high…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23498v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23498v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23498v1-abstract-full" style="display: none;">
        Exploiting low-precision computations has become a standard strategy in deep <span class="search-hit mathjax">learning</span> to address the growing computational costs imposed by ever larger models and datasets. However, naively performing all computations in low precision can lead to roundoff errors and instabilities. Therefore, mixed precision training schemes usually store the weights in high precision and use low-precision computations only for whitelisted operations. Despite their success, these principles are currently not reliable for training continuous-time architectures such as neural ordinary differential equations (Neural ODEs). This paper presents a mixed precision training framework for neural ODEs, combining explicit ODE solvers with a custom backpropagation scheme, and demonstrates its effectiveness across a range of <span class="search-hit mathjax">learning</span> tasks. Our scheme uses low-precision computations for evaluating the velocity, parameterized by the neural network, and for storing intermediate states, while stability is provided by a custom dynamic adjoint scaling and by accumulating the solution and gradients in higher precision. These contributions address two key challenges in training neural ODE: the computational cost of repeated network evaluations and the growth of memory requirements with the number of time steps or layers. Along with the paper, we publish our extendable, open-source PyTorch package rampde, whose syntax resembles that of leading packages to provide a drop-in replacement in existing codes. We demonstrate the reliability and effectiveness of our scheme using challenging test cases and on neural ODE applications in image classification and generative models, achieving approximately 50% memory reduction and up to 2x speedup while maintaining accuracy comparable to single-precision training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23498v1-abstract-full').style.display = 'none'; document.getElementById('2510.23498v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code available at https://github.com/EmoryMLIP/rampde; 26 pages, 4 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 65L06; 65G50
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2; G.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23489">arXiv:2510.23489</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23489">pdf</a>, <a href="https://arxiv.org/ps/2510.23489">ps</a>, <a href="https://arxiv.org/format/2510.23489">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantum Phase Classification of Rydberg Atom Systems Using Resource-Efficient Variational Quantum Circuits and Classical Shadows
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ahuja%2C+H">Hemish Ahuja</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bhardwaj%2C+S">Samradh Bhardwaj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dhir%2C+K">Kirti Dhir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bagdasarian%2C+R">Roman Bagdasarian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jang%2C+Z">Ziwoong Jang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23489v1-abstract-short" style="display: inline;">
        …studying many-body physics, yet distinguishing between different ordered phases without explicit order parameters remains challenging. We present a resource-efficient quantum <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> approach combining classical shadow tomography with variational quantum circuits (VQCs) for binary phase classification of Z2 an…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23489v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23489v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23489v1-abstract-full" style="display: none;">
        Quantum phase transitions in Rydberg atom arrays present significant opportunities for studying many-body physics, yet distinguishing between different ordered phases without explicit order parameters remains challenging. We present a resource-efficient quantum <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> approach combining classical shadow tomography with variational quantum circuits (VQCs) for binary phase classification of Z2 and Z3 ordered phases. Our pipeline processes 500 randomized measurements per 51-atom chain state, reconstructs shadow operators, performs PCA dimensionality reduction (514 features), and encodes features using angle embedding onto a 2-qubit parameterized circuit. The circuit employs RY-RZ angle encoding, strong entanglement via all-to-all CZ gates, and a minimal 2-parameter ansatz achieving depth 7. Training via simultaneous perturbation stochastic approximation (SPSA) with hinge loss converged in 120 iterations. The model achieved 100% test accuracy with perfect precision, recall, and F1 scores, demonstrating that minimal quantum resources suffice for high-accuracy phase classification. This work establishes pathways for quantum-enhanced condensed matter physics on near-term quantum devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23489v1-abstract-full').style.display = 'none'; document.getElementById('2510.23489v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 2 tables, and 3 figures. for associated code files, see https://github.com/Hemishahuja/FLIQ_Challenge_ClassiqDuQIS</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          81P68
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23486">arXiv:2510.23486</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23486">pdf</a>, <a href="https://arxiv.org/ps/2510.23486">ps</a>, <a href="https://arxiv.org/format/2510.23486">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Learning</span> to Reason Efficiently with Discounted Reinforcement <span class="search-hit mathjax">Learning</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ayoub%2C+A">Alex Ayoub</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Asadi%2C+K">Kavosh Asadi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schuurmans%2C+D">Dale Schuurmans</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Szepesv%C3%A1ri%2C+C">Csaba Szepesvári</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bouyarmane%2C+K">Karim Bouyarmane</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23486v1-abstract-short" style="display: inline;">
        …inflating computational cost and latency. We challenge the assumption that longer responses improve accuracy. By penalizing reasoning tokens using a discounted reinforcement <span class="search-hit mathjax">learning</span> setup (interpretable as a small token cost) and analyzing Blackwell optimality in restricted policy classes, we encourage concise yet accurate reasoning. Experiments confirm ou…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23486v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23486v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23486v1-abstract-full" style="display: none;">
        Large reasoning models (LRMs) often consume excessive tokens, inflating computational cost and latency. We challenge the assumption that longer responses improve accuracy. By penalizing reasoning tokens using a discounted reinforcement <span class="search-hit mathjax">learning</span> setup (interpretable as a small token cost) and analyzing Blackwell optimality in restricted policy classes, we encourage concise yet accurate reasoning. Experiments confirm our theoretical results that this approach shortens chains of thought while preserving accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23486v1-abstract-full').style.display = 'none'; document.getElementById('2510.23486v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23485">arXiv:2510.23485</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23485">pdf</a>, <a href="https://arxiv.org/ps/2510.23485">ps</a>, <a href="https://arxiv.org/format/2510.23485">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sefidgaran%2C+M">Milad Sefidgaran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nadjahi%2C+K">Kimia Nadjahi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaidi%2C+A">Abdellatif Zaidi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23485v1-abstract-short" style="display: inline;">
        In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical <span class="search-hit mathjax">learning</span> algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23485v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23485v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23485v1-abstract-full" style="display: none;">
        In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical <span class="search-hit mathjax">learning</span> algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of <span class="MathJax_Preview">\mathcal{O}(1/\sqrt{n})</span><script type="math/tex">\mathcal{O}(1/\sqrt{n})</script>, where <span class="MathJax_Preview">n</span><script type="math/tex">n</script> is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data "memorization" raised in those works, and which asserts that there are <span class="search-hit mathjax">learning</span> problem instances for which any <span class="search-hit mathjax">learning</span> algorithm that has good prediction there exist distributions under which the algorithm must "memorize" a big fraction of the training dataset. We show that for every <span class="search-hit mathjax">learning</span> algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23485v1-abstract-full').style.display = 'none'; document.getElementById('2510.23485v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for oral presentation at NeurIPS 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23484">arXiv:2510.23484</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23484">pdf</a>, <a href="https://arxiv.org/ps/2510.23484">ps</a>, <a href="https://arxiv.org/format/2510.23484">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Geometry">cs.CG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        T-REGS: Minimum Spanning Tree Regularization for Self-Supervised <span class="search-hit mathjax">Learning</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mordacq%2C+J">Julie Mordacq</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Loiseaux%2C+D">David Loiseaux</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kalogeiton%2C+V">Vicky Kalogeiton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oudot%2C+S">Steve Oudot</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23484v1-abstract-short" style="display: inline;">
        Self-supervised <span class="search-hit mathjax">learning</span> (SSL) has emerged as a powerful paradigm for…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23484v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23484v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23484v1-abstract-full" style="display: none;">
        Self-supervised <span class="search-hit mathjax">learning</span> (SSL) has emerged as a powerful paradigm for <span class="search-hit mathjax">learning</span> representations without labeled data, often by enforcing invariance to input transformations such as rotations or blurring. Recent studies have highlighted two pivotal properties for effective representations: (i) avoiding dimensional collapse-where the <span class="search-hit mathjax">learned</span> features occupy only a low-dimensional subspace, and (ii) enhancing uniformity of the induced distribution. In this work, we introduce T-REGS, a simple regularization framework for SSL based on the length of the Minimum Spanning Tree (MST) over the <span class="search-hit mathjax">learned</span> representation. We provide theoretical analysis demonstrating that T-REGS simultaneously mitigates dimensional collapse and promotes distribution uniformity on arbitrary compact Riemannian manifolds. Several experiments on synthetic data and on classical SSL benchmarks validate the effectiveness of our approach at enhancing representation quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23484v1-abstract-full').style.display = 'none'; document.getElementById('2510.23484v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23476">arXiv:2510.23476</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23476">pdf</a>, <a href="https://arxiv.org/ps/2510.23476">ps</a>, <a href="https://arxiv.org/format/2510.23476">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Human-AI Collaborative Uncertainty Quantification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Noorani%2C+S">Sima Noorani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kiyani%2C+S">Shayan Kiyani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pappas%2C+G">George Pappas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hassani%2C+H">Hamed Hassani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23476v1-abstract-short" style="display: inline;">
        AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative framewo…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23476v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23476v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23476v1-abstract-full" style="display: none;">
        AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a key component of reliable decision making. We introduce Human AI Collaborative Uncertainty Quantification, a framework that formalizes how an AI model can refine a human expert's proposed prediction set with two goals: avoiding counterfactual harm, ensuring the AI does not degrade correct human judgments, and complementarity, enabling recovery of correct outcomes the human missed. At the population level, we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function, extending a classical result in conformal prediction. Building on this insight, we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees. The online method adapts to distribution shifts, including human behavior evolving through interaction with AI, a phenomenon we call Human to AI Adaptation. Experiments across image classification, regression, and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone, achieving higher coverage and smaller set sizes across various conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23476v1-abstract-full').style.display = 'none'; document.getElementById('2510.23476v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23472">arXiv:2510.23472</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23472">pdf</a>, <a href="https://arxiv.org/ps/2510.23472">ps</a>, <a href="https://arxiv.org/format/2510.23472">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+K">Ke Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+R">Ruo-Tong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+R">Rong-Xi Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+X">Xi Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Y">Yunqi Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+S">Siyuan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+M">Mingxuan Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+C">Chao Qian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23472v1-abstract-short" style="display: inline;">
        Chip placement is a vital stage in modern chip design as it has a substantial impact on the subsequent processes and the overall quality of the final chip. The use of black-box optimization (BBO) for chip placement has a history of several decades. However, early efforts were limited by immature problem formulations and inefficient algorithm designs. Recent progress has shown the effectiveness and…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23472v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23472v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23472v1-abstract-full" style="display: none;">
        Chip placement is a vital stage in modern chip design as it has a substantial impact on the subsequent processes and the overall quality of the final chip. The use of black-box optimization (BBO) for chip placement has a history of several decades. However, early efforts were limited by immature problem formulations and inefficient algorithm designs. Recent progress has shown the effectiveness and efficiency of BBO for chip placement, proving its potential to achieve state-of-the-art results. Despite these advancements, the field lacks a unified, BBO-specific benchmark for thoroughly assessing various problem formulations and BBO algorithms. To fill this gap, we propose BBOPlace-Bench, the first benchmark designed specifically for evaluating and developing BBO algorithms for chip placement tasks. It integrates three problem formulations of BBO for chip placement, and offers a modular, decoupled, and flexible framework that enables users to seamlessly implement, test, and compare their own algorithms. BBOPlace-Bench integrates a wide variety of existing BBO algorithms, including simulated annealing (SA), evolutionary algorithms (EAs), and Bayesian optimization (BO). Experimental results show that the problem formulations of mask-guided optimization and hyperparameter optimization exhibit superior performance than the sequence pair problem formulation, while EAs demonstrate better overall performance than SA and BO, especially in high-dimensional search spaces, and also achieve state-of-the-art performance compared to the mainstream chip placement methods. BBOPlace-Bench not only facilitates the development of efficient BBO-driven solutions for chip placement but also broadens the practical application scenarios (which are urgently needed) for the BBO community. The code of BBOPlace-Bench is available at https://github.com/lamda-bbo/BBOPlace-Bench.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23472v1-abstract-full').style.display = 'none'; document.getElementById('2510.23472v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23471">arXiv:2510.23471</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23471">pdf</a>, <a href="https://arxiv.org/ps/2510.23471">ps</a>, <a href="https://arxiv.org/format/2510.23471">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Decision Making with Partially Calibrated Forecasts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kiyani%2C+S">Shayan Kiyani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hassani%2C+H">Hamed Hassani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pappas%2C+G">George Pappas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roth%2C+A">Aaron Roth</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23471v1-abstract-short" style="display: inline;">
        Calibration has emerged as a foundational goal in ``trustworthy <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>'', in part because of its strong decision theoretic semantics. Independent of the underlying distribution, and independent of the decision maker's utility function, calibration promises that amongst all policies mapping predict…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23471v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23471v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23471v1-abstract-full" style="display: none;">
        Calibration has emerged as a foundational goal in ``trustworthy <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>'', in part because of its strong decision theoretic semantics. Independent of the underlying distribution, and independent of the decision maker's utility function, calibration promises that amongst all policies mapping predictions to actions, the uniformly best policy is the one that ``trusts the predictions'' and acts as if they were correct. But this is true only of \emph{fully calibrated} forecasts, which are tractable to guarantee only for very low dimensional prediction problems. For higher dimensional prediction problems (e.g. when outcomes are multiclass), weaker forms of calibration have been studied that lack these decision theoretic properties. In this paper we study how a conservative decision maker should map predictions endowed with these weaker (``partial'') calibration guarantees to actions, in a way that is robust in a minimax sense: i.e. to maximize their expected utility in the worst case over distributions consistent with the calibration guarantees. We characterize their minimax optimal decision rule via a duality argument, and show that surprisingly, ``trusting the predictions and acting accordingly'' is recovered in this minimax sense by \emph{decision calibration} (and any strictly stronger notion of calibration), a substantially weaker and more tractable condition than full calibration. For calibration guarantees that fall short of decision calibration, the minimax optimal decision rule is still efficiently computable, and we provide an empirical evaluation of a natural one that applies to any regression model solved to optimize squared error.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23471v1-abstract-full').style.display = 'none'; document.getElementById('2510.23471v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23469">arXiv:2510.23469</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23469">pdf</a>, <a href="https://arxiv.org/ps/2510.23469">ps</a>, <a href="https://arxiv.org/format/2510.23469">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yuhan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+X">Xingbo Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jundong Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23469v1-abstract-short" style="display: inline;">
        In recent years, pre-training Graph Neural Networks (GNNs) through self-supervised <span class="search-hit mathjax">learning</span> on unlabeled graph data has emerged as a widely adopted paradigm in graph…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23469v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23469v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23469v1-abstract-full" style="display: none;">
        In recent years, pre-training Graph Neural Networks (GNNs) through self-supervised <span class="search-hit mathjax">learning</span> on unlabeled graph data has emerged as a widely adopted paradigm in graph <span class="search-hit mathjax">learning</span>. Although the paradigm is effective for pre-training powerful GNN models, the objective gap often exists between pre-training and downstream tasks. To bridge this gap, graph prompting adapts pre-trained GNN models to specific downstream tasks with extra learnable prompts while keeping the pre-trained GNN models frozen. As recent graph prompting methods largely focus on enhancing model utility on downstream tasks, they often overlook fairness concerns when designing prompts for adaptation. In fact, pre-trained GNN models will produce discriminative node representations across demographic subgroups, as downstream graph data inherently contains biases in both node attributes and graph structures. To address this issue, we propose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairness for adapting pre-trained GNN models to downstream tasks. To mitigate attribute bias, we design an Adaptive Feature Rectification module that <span class="search-hit mathjax">learns</span> customized attribute prompts to suppress sensitive information at the input layer, reducing bias at the source. Afterward, we propose an Adaptive Message Calibration module that generates structure prompts at each layer, which adjust the message from neighboring nodes to enable dynamic and soft calibration of the information flow. Finally, ADPrompt jointly optimizes the two prompting modules to adapt the pre-trained GNN while enhancing fairness. We conduct extensive experiments on four datasets with four pre-training strategies to evaluate the performance of ADPrompt. The results demonstrate that our proposed ADPrompt outperforms seven baseline methods on node classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23469v1-abstract-full').style.display = 'none'; document.getElementById('2510.23469v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23463">arXiv:2510.23463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23463">pdf</a>, <a href="https://arxiv.org/ps/2510.23463">ps</a>, <a href="https://arxiv.org/format/2510.23463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Differential Privacy as a Perk: Federated <span class="search-hit mathjax">Learning</span> over Multiple-Access Fading Channels with a Multi-Antenna Base Station
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+H">Hao Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+H">Haifeng Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+K">Kaishun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xing%2C+H">Hong Xing</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23463v1-abstract-short" style="display: inline;">
        Federated <span class="search-hit mathjax">Learning</span> (FL) is a distributed <span class="search-hit mathjax">learning</span> paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \emph{over-the-air FL (AirFL)},…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23463v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23463v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23463v1-abstract-full" style="display: none;">
        Federated <span class="search-hit mathjax">Learning</span> (FL) is a distributed <span class="search-hit mathjax">learning</span> paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \emph{over-the-air FL (AirFL)}, the inherent channel noise plays a unique role of \emph{frenemy} in the sense that it degrades training due to noisy global aggregation while providing a natural source of randomness for privacy-preserving mechanisms, formally quantified by \emph{differential privacy (DP)}. It remains, nevertheless, challenging to effectively harness such channel impairments, as prior arts, under assumptions of either simple channel models or restricted types of loss functions, mostly considering (local) DP enhancement with a single-round or non-convergent bound on privacy loss. In this paper, we study AirFL over multiple-access fading channels with a multi-antenna base station (BS) subject to user-level DP requirements. Despite a recent study, which claimed in similar settings that artificial noise (AN) must be injected to ensure DP in general, we demonstrate, on the contrary, that DP can be gained as a \emph{perk} even \emph{without} employing any AN. Specifically, we derive a novel bound on DP that converges under general bounded-domain assumptions on model parameters, along with a convergence bound with general smooth and non-convex loss functions. Next, we optimize over receive beamforming and power allocations to characterize the optimal convergence-privacy trade-offs, which also reveal explicit conditions in which DP is achievable without compromising training. Finally, our theoretical findings are validated by extensive numerical results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23463v1-abstract-full').style.display = 'none'; document.getElementById('2510.23463v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 5 figures, submitted for possible publication</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23455">arXiv:2510.23455</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23455">pdf</a>, <a href="https://arxiv.org/ps/2510.23455">ps</a>, <a href="https://arxiv.org/format/2510.23455">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SGFusion: Stochastic Geographic Gradient Fusion in Federated <span class="search-hit mathjax">Learning</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+K">Khoa Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tran%2C+K">Khang Tran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Phan%2C+N">NhatHai Phan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Borcea%2C+C">Cristian Borcea</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+R">Rouming Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khalil%2C+I">Issa Khalil</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23455v1-abstract-short" style="display: inline;">
        This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel training algorithm to leverage the geographic information of mobile users in Federated <span class="search-hit mathjax">Learning</span> (FL). SGFusion maps the data collected by mobile devices onto geographical zones and trains one FL model per zone, which adapts well to the data and behaviors of users in that zone. SGFu…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23455v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23455v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23455v1-abstract-full" style="display: none;">
        This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel training algorithm to leverage the geographic information of mobile users in Federated <span class="search-hit mathjax">Learning</span> (FL). SGFusion maps the data collected by mobile devices onto geographical zones and trains one FL model per zone, which adapts well to the data and behaviors of users in that zone. SGFusion models the local data-based correlation among geographical zones as a hierarchical random graph (HRG) optimized by Markov Chain Monte Carlo sampling. At each training step, every zone fuses its local gradient with gradients derived from a small set of other zones sampled from the HRG. This approach enables knowledge fusion and sharing among geographical zones in a probabilistic and stochastic gradient fusion process with self-attention weights, such that "more similar" zones have "higher probabilities" of sharing gradients with "larger attention weights." SGFusion remarkably improves model utility without introducing undue computational cost. Extensive theoretical and empirical results using a heart-rate prediction dataset collected across 6 countries show that models trained with SGFusion converge with upper-bounded expected errors and significantly improve utility in all countries compared to existing approaches without notable cost in system scalability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23455v1-abstract-full').style.display = 'none'; document.getElementById('2510.23455v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23449">arXiv:2510.23449</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23449">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Schrodinger Neural Network and Uncertainty Quantification: Quantum <span class="search-hit mathjax">Machine</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hammad%2C+M+M">M. M. Hammad</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23449v1-abstract-short" style="display: inline;">
        …input to a normalized wave function on the output domain and computes predictive probabilities via the Born rule. The SNN departs from standard parametric likelihood heads by <span class="search-hit mathjax">learning</span> complex coefficients of a spectral expansion (e . g ., Chebyshev polynomials) whose squared modulus yields the conditional density <span class="MathJax_Preview">p(y|x)=\left| ψ_x(y)\right| {}^2</span><script type="math/tex">p(y|x)=\left| ψ_x(y)\right| {}^2</script> with analy…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23449v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23449v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23449v1-abstract-full" style="display: none;">
        We introduce the Schrodinger Neural Network (SNN), a principled architecture for conditional density estimation and uncertainty quantification inspired by quantum mechanics. The SNN maps each input to a normalized wave function on the output domain and computes predictive probabilities via the Born rule. The SNN departs from standard parametric likelihood heads by <span class="search-hit mathjax">learning</span> complex coefficients of a spectral expansion (e . g ., Chebyshev polynomials) whose squared modulus yields the conditional density <span class="MathJax_Preview">p(y|x)=\left| ψ_x(y)\right| {}^2</span><script type="math/tex">p(y|x)=\left| ψ_x(y)\right| {}^2</script> with analytic normalization. This representation confers three practical advantages: positivity and exact normalization by construction, native multimodality through interference among basis modes without explicit mixture bookkeeping, and yields closed-form (or efficiently computable) functionals<span class="MathJax_Preview">-</span><script type="math/tex">-</script>such as moments and several calibration diagnostics<span class="MathJax_Preview">-</span><script type="math/tex">-</script>as quadratic forms in coefficient space. We develop the statistical and computational foundations of the SNN, including (i) training by exact maximum-likelihood with unit-sphere coefficient parameterization, (ii) physics-inspired quadratic regularizers (kinetic and potential energies) motivated by uncertainty relations between localization and spectral complexity, (iii) scalable low-rank and separable extensions for multivariate outputs, (iv) operator-based extensions that represent observables, constraints, and weak labels as self-adjoint matrices acting on the amplitude space, and (v) a comprehensive framework for evaluating multimodal predictions. The SNN provides a coherent, tractable framework to elevate probabilistic prediction from point estimates to physically inspired amplitude-based distributions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23449v1-abstract-full').style.display = 'none'; document.getElementById('2510.23449v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">29 pages, 16 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23448">arXiv:2510.23448</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23448">pdf</a>, <a href="https://arxiv.org/ps/2510.23448">ps</a>, <a href="https://arxiv.org/format/2510.23448">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Information-Theoretic Analysis of Out-of-Distribution Generalization in Meta-<span class="search-hit mathjax">Learning</span> with Applications to Meta-RL
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xingtu Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23448v1-abstract-short" style="display: inline;">
        In this work, we study out-of-distribution generalization in meta-<span class="search-hit mathjax">learning</span> from an information-theoretic perspective. We focus on two scenarios: (i) when the testing environment mismatches the training environment, and (ii) when the training environment is broader than the testing environment. The first corresponds to the standard distribution mismatch setti…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23448v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23448v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23448v1-abstract-full" style="display: none;">
        In this work, we study out-of-distribution generalization in meta-<span class="search-hit mathjax">learning</span> from an information-theoretic perspective. We focus on two scenarios: (i) when the testing environment mismatches the training environment, and (ii) when the training environment is broader than the testing environment. The first corresponds to the standard distribution mismatch setting, while the second reflects a broad-to-narrow training scenario. We further formalize the generalization problem in meta-reinforcement <span class="search-hit mathjax">learning</span> and establish corresponding generalization bounds. Finally, we analyze the generalization performance of a gradient-based meta-reinforcement <span class="search-hit mathjax">learning</span> algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23448v1-abstract-full').style.display = 'none'; document.getElementById('2510.23448v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23438">arXiv:2510.23438</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23438">pdf</a>, <a href="https://arxiv.org/ps/2510.23438">ps</a>, <a href="https://arxiv.org/format/2510.23438">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Geometry">cs.CG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Coresets for Clustering Under Stochastic Noise
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lingxiao Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhize Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vishnoi%2C+N+K">Nisheeth K. Vishnoi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+R">Runkai Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Haoyu Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23438v1-abstract-short" style="display: inline;">
        We study the problem of constructing coresets for <span class="MathJax_Preview">(k, z)</span><script type="math/tex">(k, z)</script>-clustering when the input dataset is corrupted by stochastic noise drawn from a known distribution. In this setting, evaluating the quality of a coreset is inherently challenging, as the true underlying dataset is unobserved. To address this, we investigate coreset construction using surrogate error metrics that are tractable and provably…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23438v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23438v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23438v1-abstract-full" style="display: none;">
        We study the problem of constructing coresets for <span class="MathJax_Preview">(k, z)</span><script type="math/tex">(k, z)</script>-clustering when the input dataset is corrupted by stochastic noise drawn from a known distribution. In this setting, evaluating the quality of a coreset is inherently challenging, as the true underlying dataset is unobserved. To address this, we investigate coreset construction using surrogate error metrics that are tractable and provably related to the true clustering cost. We analyze a traditional metric from prior work and introduce a new error metric that more closely aligns with the true cost. Although our metric is defined independently of the noise distribution, it enables approximation guarantees that scale with the noise level. We design a coreset construction algorithm based on this metric and show that, under mild assumptions on the data and noise, enforcing an <span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script>-bound under our metric yields smaller coresets and tighter guarantees on the true clustering cost than those obtained via classical metrics. In particular, we prove that the coreset size can improve by a factor of up to <span class="MathJax_Preview">\mathrm{poly}(k)</span><script type="math/tex">\mathrm{poly}(k)</script>, where <span class="MathJax_Preview">n</span><script type="math/tex">n</script> is the dataset size. Experiments on real-world datasets support our theoretical findings and demonstrate the practical advantages of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23438v1-abstract-full').style.display = 'none'; document.getElementById('2510.23438v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted by NeurIPS 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23428">arXiv:2510.23428</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23428">pdf</a>, <a href="https://arxiv.org/ps/2510.23428">ps</a>, <a href="https://arxiv.org/format/2510.23428">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1021/acs.jcim.5c01844">10.1021/acs.jcim.5c01844 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Predictions of Molecular Properties with Graph Featurisation and Heterogeneous Ensemble Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Parker%2C+M+L">Michael L. Parker</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mahmoud%2C+S">Samar Mahmoud</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Montefiore%2C+B">Bailey Montefiore</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=%C3%96eren%2C+M">Mario Öeren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tandon%2C+H">Himani Tandon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wharrick%2C+C">Charlotte Wharrick</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Segall%2C+M+D">Matthew D. Segall</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23428v1-abstract-short" style="display: inline;">
        We explore a "best-of-both" approach to modelling molecular properties by combining <span class="search-hit mathjax">learned</span> molecular descriptors from a graph neural network (GNN) with general-purpose descriptors and a mixed ensemble of…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23428v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23428v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23428v1-abstract-full" style="display: none;">
        We explore a "best-of-both" approach to modelling molecular properties by combining <span class="search-hit mathjax">learned</span> molecular descriptors from a graph neural network (GNN) with general-purpose descriptors and a mixed ensemble of <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> (ML) models. We introduce a MetaModel framework to aggregate predictions from a diverse set of leading ML models. We present a featurisation scheme for combining task-specific GNN-derived features with conventional molecular descriptors.
  We demonstrate that our framework outperforms the cutting-edge ChemProp model on all regression datasets tested and 6 of 9 classification datasets. We further show that including the GNN features derived from ChemProp boosts the ensemble model's performance on several datasets where it otherwise would have underperformed. We conclude that to achieve optimal performance across a wide set of problems, it is vital to combine general-purpose descriptors with task-specific <span class="search-hit mathjax">learned</span> features and use a diverse set of ML models to make the predictions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23428v1-abstract-full').style.display = 'none'; document.getElementById('2510.23428v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        JCIM (2025)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23427">arXiv:2510.23427</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23427">pdf</a>, <a href="https://arxiv.org/ps/2510.23427">ps</a>, <a href="https://arxiv.org/format/2510.23427">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PrivacyGuard: A Modular Framework for Privacy Auditing in <span class="search-hit mathjax">Machine</span> <span class="search-hit mathjax">Learning</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Melis%2C+L">Luca Melis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grange%2C+M">Matthew Grange</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kalemaj%2C+I">Iden Kalemaj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chadha%2C+K">Karan Chadha</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+S">Shengyuan Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kashtelyan%2C+E">Elena Kashtelyan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bullock%2C+W">Will Bullock</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23427v1-abstract-short" style="display: inline;">
        The increasing deployment of <span class="search-hit mathjax">Machine</span> <span class="search-hit mathjax">Learning</span> (ML) models in sensitive domains motivates the need for robust, practical privacy assessment tools. PrivacyGuard is a comprehensive tool for empirical differential privacy (DP) analysis, designed to evaluate privacy risks in ML models through state-of-the-art inference atta…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23427v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23427v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23427v1-abstract-full" style="display: none;">
        The increasing deployment of <span class="search-hit mathjax">Machine</span> <span class="search-hit mathjax">Learning</span> (ML) models in sensitive domains motivates the need for robust, practical privacy assessment tools. PrivacyGuard is a comprehensive tool for empirical differential privacy (DP) analysis, designed to evaluate privacy risks in ML models through state-of-the-art inference attacks and advanced privacy measurement techniques. To this end, PrivacyGuard implements a diverse suite of privacy attack-- including membership inference , extraction, and reconstruction attacks -- enabling both off-the-shelf and highly configurable privacy analyses. Its modular architecture allows for the seamless integration of new attacks, and privacy metrics, supporting rapid adaptation to emerging research advances. We make PrivacyGuard available at https://github.com/facebookresearch/PrivacyGuard.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23427v1-abstract-full').style.display = 'none'; document.getElementById('2510.23427v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23409">arXiv:2510.23409</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23409">pdf</a>, <a href="https://arxiv.org/ps/2510.23409">ps</a>, <a href="https://arxiv.org/format/2510.23409">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+Y">Youngjun Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+J">Joonseong Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lim%2C+S">Sungjun Lim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+K">Kyungwoo Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23409v1-abstract-short" style="display: inline;">
        Data valuation has become central in the era of data-centric AI. It drives efficient training pipelines and enables objective pricing in data markets by assigning a numeric value to each data point. Most existing data valuation methods estimate the effect of removing individual data points by evaluating changes in model validation performance under in-distribution (ID) settings, as opposed to out-…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23409v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23409v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23409v1-abstract-full" style="display: none;">
        Data valuation has become central in the era of data-centric AI. It drives efficient training pipelines and enables objective pricing in data markets by assigning a numeric value to each data point. Most existing data valuation methods estimate the effect of removing individual data points by evaluating changes in model validation performance under in-distribution (ID) settings, as opposed to out-of-distribution (OOD) scenarios where data follow different patterns. Since ID and OOD data behave differently, data valuation methods based on ID loss often fail to generalize to OOD settings, particularly when the validation set contains no OOD data. Furthermore, although OOD-aware methods exist, they involve heavy computational costs, which hinder practical deployment. To address these challenges, we introduce \emph{Eigen-Value} (EV), a plug-and-play data valuation framework for OOD robustness that uses only an ID data subset, including during validation. EV provides a new spectral approximation of domain discrepancy, which is the gap of loss between ID and OOD using ratios of eigenvalues of ID data's covariance matrix. EV then estimates the marginal contribution of each data point to this discrepancy via perturbation theory, alleviating the computational burden. Subsequently, EV plugs into ID loss-based methods by adding an EV term without any additional training loop. We demonstrate that EV achieves improved OOD robustness and stable value rankings across real-world datasets, while remaining computationally lightweight. These results indicate that EV is practical for large-scale settings with domain shift, offering an efficient path to OOD-robust data valuation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23409v1-abstract-full').style.display = 'none'; document.getElementById('2510.23409v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23408">arXiv:2510.23408</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23408">pdf</a>, <a href="https://arxiv.org/ps/2510.23408">ps</a>, <a href="https://arxiv.org/format/2510.23408">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Younesi%2C+A">Abolfazl Younesi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Samani%2C+Z+N">Zahra Najafabadi Samani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fahringer%2C+T">Thomas Fahringer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23408v1-abstract-short" style="display: inline;">
        Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23408v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23408v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23408v1-abstract-full" style="display: none;">
        Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23408v1-abstract-full').style.display = 'none'; document.getElementById('2510.23408v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23393">arXiv:2510.23393</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23393">pdf</a>, <a href="https://arxiv.org/ps/2510.23393">ps</a>, <a href="https://arxiv.org/format/2510.23393">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Best of N Worlds: Aligning Reinforcement <span class="search-hit mathjax">Learning</span> with Best-of-N Sampling via max@k Optimisation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bagirov%2C+F">Farid Bagirov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arkhipov%2C+M">Mikhail Arkhipov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sycheva%2C+K">Ksenia Sycheva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Glukhov%2C+E">Evgeniy Glukhov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bogomolov%2C+E">Egor Bogomolov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23393v1-abstract-short" style="display: inline;">
        The application of Reinforcement <span class="search-hit mathjax">Learning</span> with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement <span class="search-hit mathjax">learning</span> fine-tuning…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23393v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23393v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23393v1-abstract-full" style="display: none;">
        The application of Reinforcement <span class="search-hit mathjax">Learning</span> with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement <span class="search-hit mathjax">learning</span> fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values. In this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k. We derive an unbiased on-policy gradient estimate for direct optimization of this metric. Furthermore, we extend our derivations to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency. Empirically, we show that our objective effectively optimizes max@k metric in off-policy scenarios, aligning the model with the Best-of-N inference strategy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23393v1-abstract-full').style.display = 'none'; document.getElementById('2510.23393v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23389">arXiv:2510.23389</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23389">pdf</a>, <a href="https://arxiv.org/ps/2510.23389">ps</a>, <a href="https://arxiv.org/format/2510.23389">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Floating-Point Neural Network Verification at the Software Level
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Manino%2C+E">Edoardo Manino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Farias%2C+B">Bruno Farias</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Menezes%2C+R+S">Rafael Sá Menezes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shmarov%2C+F">Fedor Shmarov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cordeiro%2C+L+C">Lucas C. Cordeiro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23389v1-abstract-short" style="display: inline;">
        The behaviour of neural network components must be proven correct before deployment in safety-critical systems. Unfortunately, existing neural network verification techniques cannot certify the absence of faults at the software level. In this paper, we show how to specify and verify that neural networks are safe, by explicitly reasoning about their floating-point implementation. In doing so, we co…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23389v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23389v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23389v1-abstract-full" style="display: none;">
        The behaviour of neural network components must be proven correct before deployment in safety-critical systems. Unfortunately, existing neural network verification techniques cannot certify the absence of faults at the software level. In this paper, we show how to specify and verify that neural networks are safe, by explicitly reasoning about their floating-point implementation. In doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural network verification examples that cover activation functions, common layers, and full neural networks of up to 170K parameters. Our verification suite is written in plain C and is compatible with the format of the International Competition on Software Verification (SV-COMP). Thanks to it, we can conduct the first rigorous evaluation of eight state-of-the-art software verifiers on neural network code. The results show that existing automated verification tools can correctly solve an average of 11% of our benchmark, while producing around 3% incorrect verdicts. At the same time, a historical analysis reveals that the release of our benchmark has already had a significantly positive impact on the latter.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23389v1-abstract-full').style.display = 'none'; document.getElementById('2510.23389v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Pre-print before submission to peer review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23384">arXiv:2510.23384</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23384">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.5281/ZENODO.17389005">10.5281/ZENODO.17389005 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kalamkar%2C+P+N">Pratik N. Kalamkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Phakatkar%2C+A+G">A. G. Phakatkar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23384v1-abstract-short" style="display: inline;">
        Opinions are central to almost all human activities and are key influencers of our behaviors. In current times due to growth of social networking website and increase in number of e-commerce site huge amount of opinions are now available on web. Given a set of evaluative statements that contain opinions (or sentiments) about an Entity, opinion mining aims to extract attributes and components of th…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23384v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23384v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23384v1-abstract-full" style="display: none;">
        Opinions are central to almost all human activities and are key influencers of our behaviors. In current times due to growth of social networking website and increase in number of e-commerce site huge amount of opinions are now available on web. Given a set of evaluative statements that contain opinions (or sentiments) about an Entity, opinion mining aims to extract attributes and components of the object that have been commented on in each statement and to determine whether the comments are positive, negative or neutral. While lot of research recently has been done in field of opinion mining and some of it dealing with ranking of entities based on review or opinion set, classifying opinions into finer granularity level and then ranking entities has never been done before. In this paper method for opinion mining from statements at a deeper level of granularity is proposed. This is done by using fuzzy logic reasoning, after which entities are ranked as per this information.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23384v1-abstract-full').style.display = 'none'; document.getElementById('2510.23384v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 4 figures, Conference Paper</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        (2014). Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach. Elsevier
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23379">arXiv:2510.23379</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23379">pdf</a>, <a href="https://arxiv.org/ps/2510.23379">ps</a>, <a href="https://arxiv.org/format/2510.23379">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Symbolic Neural Generation with Applications to Lead Discovery in Drug Design
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Srinivasan%2C+A">Ashwin Srinivasan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Baskar%2C+A">A Baskar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dash%2C+T">Tirtharaj Dash</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bain%2C+M">Michael Bain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dey%2C+S+K">Sanjay Kumar Dey</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Banerjee%2C+M">Mainak Banerjee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23379v1-abstract-short" style="display: inline;">
        We investigate a relatively underexplored class of hybrid neurosymbolic models integrating symbolic <span class="search-hit mathjax">learning</span> with neural reasoning to construct data generators meeting formal correctness criteria. In \textit{Symbolic Neural Generators} (SNGs), symbolic learners examine logical specifications of feasible data from a small set of instances -- sometimes just on…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23379v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23379v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23379v1-abstract-full" style="display: none;">
        We investigate a relatively underexplored class of hybrid neurosymbolic models integrating symbolic <span class="search-hit mathjax">learning</span> with neural reasoning to construct data generators meeting formal correctness criteria. In \textit{Symbolic Neural Generators} (SNGs), symbolic learners examine logical specifications of feasible data from a small set of instances -- sometimes just one. Each specification in turn constrains the conditional information supplied to a neural-based generator, which rejects any instance violating the symbolic specification. Like other neurosymbolic approaches, SNG exploits the complementary strengths of symbolic and neural methods. The outcome of an SNG is a triple <span class="MathJax_Preview">(H, X, W)</span><script type="math/tex">(H, X, W)</script>, where <span class="MathJax_Preview">H</span><script type="math/tex">H</script> is a symbolic description of feasible instances constructed from data, <span class="MathJax_Preview">X</span><script type="math/tex">X</script> a set of generated new instances that satisfy the description, and <span class="MathJax_Preview">W</span><script type="math/tex">W</script> an associated weight. We introduce a semantics for such systems, based on the construction of appropriate \textit{base} and \textit{fibre} partially-ordered sets combined into an overall partial order, and outline a probabilistic extension relevant to practical applications. In this extension, SNGs result from searching over a weighted partial ordering. We implement an SNG combining a restricted form of Inductive Logic Programming (ILP) with a large language model (LLM) and evaluate it on early-stage drug design. Our main interest is the description and the set of potential inhibitor molecules generated by the SNG. On benchmark problems -- where drug targets are well understood -- SNG performance is statistically comparable to state-of-the-art methods. On exploratory problems with poorly understood targets, generated molecules exhibit binding affinities on par with leading clinical candidates. Experts further find the symbolic specifications useful as preliminary filters, with several generated molecules identified as viable for synthesis and wet-lab testing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23379v1-abstract-full').style.display = 'none'; document.getElementById('2510.23379v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">37 pages, 15 figures; partial overlap of experimental results with https://doi.org/10.1101/2025.02.14.634875</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6; I.2.1; J.3
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23371">arXiv:2510.23371</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23371">pdf</a>, <a href="https://arxiv.org/ps/2510.23371">ps</a>, <a href="https://arxiv.org/format/2510.23371">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">Hyunseung Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jeong%2C+D">Dae-Woong Jeong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+C">Changyoung Park</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+W">Won-Ji Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+H">Ha-Eun Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Ji-Hye Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hormazabal%2C+R">Rodrigo Hormazabal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ko%2C+S+M">Sung Moon Ko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+S">Sumin Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yim%2C+S">Soorin Yim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+C">Chanhui Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+S">Sehui Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cha%2C+S">Sang-Ho Cha</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lim%2C+W">Woohyung Lim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23371v1-abstract-short" style="display: inline;">
        …data collection and retraining for each new property. Here we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a generalizable AI framework that jointly <span class="search-hit mathjax">learns</span> 34 physicochemical properties spanning thermal, electrical, mechanical, and optical domains. By aligning these properties within a shared geometric space, GATE captures cross-pr…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23371v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23371v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23371v1-abstract-full" style="display: none;">
        Artificial intelligence (AI) has emerged as a powerful accelerator of materials discovery, yet most existing models remain problem-specific, requiring additional data collection and retraining for each new property. Here we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a generalizable AI framework that jointly <span class="search-hit mathjax">learns</span> 34 physicochemical properties spanning thermal, electrical, mechanical, and optical domains. By aligning these properties within a shared geometric space, GATE captures cross-property correlations that reduce disjoint-property bias -- a key factor causing false negatives in multi-criteria screening. To demonstrate its generalizability, GATE -- without any problem-specific reconfiguration -- was directly applied to the discovery of immersion cooling fluids for data centers, a stringent real-world challenge defined by the Open Compute Project (OCP). Screening billions of candidates, GATE identified 92,861 molecules as promising for practical deployment. Four were experimentally or literarily validated, showing strong agreement with wet-lab measurements and performance comparable to or exceeding a commercial coolant. These results establish GATE as a ready-to-use, generalizable AI platform readily applicable across diverse materials discovery tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23371v1-abstract-full').style.display = 'none'; document.getElementById('2510.23371v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23364">arXiv:2510.23364</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23364">pdf</a>, <a href="https://arxiv.org/ps/2510.23364">ps</a>, <a href="https://arxiv.org/format/2510.23364">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">Hyeongkyun Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oikonomou%2C+O">Orestis Oikonomou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23364v1-abstract-short" style="display: inline;">
        …Sentinel-1 or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich regions, ZeroFlood bridges data availability gaps through cross-modal representation <span class="search-hit mathjax">learning</span>. Experiments with TerraMind and Prithvi GFMs show that TiM enhances model robustness, with the TerraMind-Large configuration achieving an F1 score of 67.21. The results demonst…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23364v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23364v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23364v1-abstract-full" style="display: none;">
        Flood susceptibility mapping (FSM) is vital for disaster prevention but remains challenging in data-scarce regions where hydrodynamic models require dense geophysical inputs. This work introduces ZeroFlood, a geospatial foundation model framework for data-efficient FSM. The approach fine-tunes Geospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning, enabling flood prediction from basic Earth observation data such as Sentinel-1 or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich regions, ZeroFlood bridges data availability gaps through cross-modal representation <span class="search-hit mathjax">learning</span>. Experiments with TerraMind and Prithvi GFMs show that TiM enhances model robustness, with the TerraMind-Large configuration achieving an F1 score of 67.21. The results demonstrate the feasibility of foundation-model-based FSM as a scalable and data-efficient solution for flood risk management.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23364v1-abstract-full').style.display = 'none'; document.getElementById('2510.23364v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint submitted to EUSAR 2026 (under review)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23362">arXiv:2510.23362</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23362">pdf</a>, <a href="https://arxiv.org/ps/2510.23362">ps</a>, <a href="https://arxiv.org/format/2510.23362">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Non-negative Proximal Gradient Algorithm for Inverse Problems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hanzhang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zonglin Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+J">Jingyi Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chenyang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Z">Zhiwei Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Q">Qiangqiang Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23362v1-abstract-short" style="display: inline;">
        …and derive its SSO-PGA-based optimization algorithm, which is then unfolded into a deep network to marry the interpretability of optimization with the power of deep <span class="search-hit mathjax">learning</span>. Extensive numerical and real-world experiments demonstrate that our method significantly surpasses traditional PGA and other state-of-the-art algorithms, ensuring superior performance a…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23362v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23362v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23362v1-abstract-full" style="display: none;">
        Proximal gradient algorithms (PGA), while foundational for inverse problems like image reconstruction, often yield unstable convergence and suboptimal solutions by violating the critical non-negativity constraint. We identify the gradient descent step as the root cause of this issue, which introduces negative values and induces high sensitivity to hyperparameters. To overcome these limitations, we propose a novel multiplicative update proximal gradient algorithm (SSO-PGA) with convergence guarantees, which is designed for robustness in non-negative inverse problems. Our key innovation lies in superseding the gradient descent step with a learnable sigmoid-based operator, which inherently enforces non-negativity and boundedness by transforming traditional subtractive updates into multiplicative ones. This design, augmented by a sliding parameter for enhanced stability and convergence, not only improves robustness but also boosts expressive capacity and noise immunity. We further formulate a degradation model for multi-modal restoration and derive its SSO-PGA-based optimization algorithm, which is then unfolded into a deep network to marry the interpretability of optimization with the power of deep <span class="search-hit mathjax">learning</span>. Extensive numerical and real-world experiments demonstrate that our method significantly surpasses traditional PGA and other state-of-the-art algorithms, ensuring superior performance and stability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23362v1-abstract-full').style.display = 'none'; document.getElementById('2510.23362v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23347">arXiv:2510.23347</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23347">pdf</a>, <a href="https://arxiv.org/ps/2510.23347">ps</a>, <a href="https://arxiv.org/format/2510.23347">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Econometrics">econ.EM</span>
        
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applications">stat.AP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Macroeconomic Forecasting for the G7 countries under Uncertainty Shocks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sengupta%2C+S">Shovon Sengupta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Singh%2C+S+K">Sunny Kumar Singh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chakraborty%2C+T">Tanujit Chakraborty</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23347v1-abstract-short" style="display: inline;">
        …(state dependent impulse responses). Out-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms 14 benchmarks, including classical VARs and leading <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> models, as confirmed by Murphy difference diagrams, multivariate Diebold Mariano tests, and Giacomini White predictability tests. Cred…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23347v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23347v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23347v1-abstract-full" style="display: none;">
        Accurate macroeconomic forecasting has become harder amid geopolitical disruptions, policy reversals, and volatile financial markets. Conventional vector autoregressions (VARs) overfit in high dimensional settings, while threshold VARs struggle with time varying interdependencies and complex parameter structures. We address these limitations by extending the Sims Zha Bayesian VAR with exogenous variables (SZBVARx) to incorporate domain-informed shrinkage and four newspaper based uncertainty shocks such as economic policy uncertainty, geopolitical risk, US equity market volatility, and US monetary policy uncertainty. The framework improves structural interpretability, mitigates dimensionality, and imposes empirically guided regularization. Using G7 data, we study spillovers from uncertainty shocks to five core variables (unemployment, real broad effective exchange rates, short term rates, oil prices, and CPI inflation), combining wavelet coherence (time frequency dynamics) with nonlinear local projections (state dependent impulse responses). Out-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms 14 benchmarks, including classical VARs and leading <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span> models, as confirmed by Murphy difference diagrams, multivariate Diebold Mariano tests, and Giacomini White predictability tests. Credible Bayesian prediction intervals deliver robust uncertainty quantification for scenario analysis and risk management. The proposed SZBVARx offers G7 policymakers a transparent, well calibrated tool for modern macroeconomic forecasting under pervasive uncertainty.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23347v1-abstract-full').style.display = 'none'; document.getElementById('2510.23347v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23346">arXiv:2510.23346</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23346">pdf</a>, <a href="https://arxiv.org/ps/2510.23346">ps</a>, <a href="https://arxiv.org/format/2510.23346">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xinyu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=K%C3%BCbler%2C+J+M">Jonas M. Kübler</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Budhathoki%2C+K">Kailash Budhathoki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yida Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kleindessner%2C+M">Matthäus Kleindessner</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23346v1-abstract-short" style="display: inline;">
        When serving a single base LLM with several different LoRA adapters simultaneously, the adapters cannot simply be merged with the base model's weights as the adapter swapping would create overhead and requests using different adapters could not be batched. Rather, the LoRA computations have to be separated from the base LLM computations, and in a multi-device setup the LoRA adapters can be sharded…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23346v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23346v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23346v1-abstract-full" style="display: none;">
        When serving a single base LLM with several different LoRA adapters simultaneously, the adapters cannot simply be merged with the base model's weights as the adapter swapping would create overhead and requests using different adapters could not be batched. Rather, the LoRA computations have to be separated from the base LLM computations, and in a multi-device setup the LoRA adapters can be sharded in a way that is well aligned with the base model's tensor parallel execution, as proposed in S-LoRA. However, the S-LoRA sharding strategy encounters some communication overhead, which may be small in theory, but can be large in practice. In this paper, we propose to constrain certain LoRA factors to be block-diagonal, which allows for an alternative way of sharding LoRA adapters that does not require any additional communication for the LoRA computations. We demonstrate in extensive experiments that our block-diagonal LoRA approach is similarly parameter efficient as standard LoRA (i.e., for a similar number of parameters it achieves similar downstream performance) and that it leads to significant end-to-end speed-up over S-LoRA. For example, when serving on eight A100 GPUs, we observe up to 1.79x (1.23x) end-to-end speed-up with 0.87x (1.74x) the number of adapter parameters for Llama-3.1-70B, and up to 1.63x (1.3x) end-to-end speed-up with 0.86x (1.73x) the number of adapter parameters for Llama-3.1-8B.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23346v1-abstract-full').style.display = 'none'; document.getElementById('2510.23346v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23342">arXiv:2510.23342</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23342">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reciprocity Deficits: Observing AI in the street with everyday publics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Taylor%2C+A+S">Alex S. Taylor</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Marres%2C+N">Noortje Marres</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bunz%2C+M">Mercedes Bunz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Phan%2C+T">Thao Phan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ganesh%2C+M+I">Maya Indira Ganesh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barron%2C+D">Dominique Barron</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boudiaf%2C+Y">Yasmine Boudiaf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Coldicutt%2C+R">Rachel Coldicutt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Emsley%2C+I">Iain Emsley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gobbo%2C+B">Beatrice Gobbo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hickman%2C+L">Louise Hickman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nissen%2C+B">Bettina Nissen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Patel%2C+M">Mukul Patel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soares%2C+L">Luis Soares</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23342v1-abstract-short" style="display: inline;">
        The street has emerged as a primary site where everyday publics are confronted with AI as an infrastructural phenomenon, as <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>-based systems are now commonly deployed in this setting in the form of automated cars, facial recognition, smart billboards and the like. While these deployments of AI in the stre…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23342v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23342v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23342v1-abstract-full" style="display: none;">
        The street has emerged as a primary site where everyday publics are confronted with AI as an infrastructural phenomenon, as <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>-based systems are now commonly deployed in this setting in the form of automated cars, facial recognition, smart billboards and the like. While these deployments of AI in the street have attracted significant media attention and public controversy in recent years, the presence of AI in the street often remains inscrutable, and many everyday publics are unaware of it. In this paper, we explore the challenges and possibilities of everyday public engagement with AI in the situated environment of city streets under these paradoxical conditions. Combining perspectives and approaches from social and cultural studies of AI, Design Research and Science and Technology Studies (STS), we explore the affordances of the street as a site for 'material participation' in AI through design-based interventions: the creation of 'everyday AI observatories.' We narrate and reflect on our participatory observations of AI in five city streets in the UK and Australia and highlight a set of tensions that emerged from them: 1) the framing of the street as a transactional environment, 2) the designed invisibility of AI and its publics in the street 3) the stratification of street environments through statistical governance. Based on this discussion and drawing on Jane Jacobs' notion of "eyes on the street," we put forward the relational notion of "reciprocity deficits" between AI infrastructures and everyday publics in the street. The conclusion reflects on the consequences of this form of social invisibility of AI for situated engagement with AI by everyday publics in the street and for public trust in urban governance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23342v1-abstract-full').style.display = 'none'; document.getElementById('2510.23342v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 4 Figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2510.23330">arXiv:2510.23330</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2510.23330">pdf</a>, <a href="https://arxiv.org/ps/2510.23330">ps</a>, <a href="https://arxiv.org/format/2510.23330">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Astrophysics of Galaxies">astro-ph.GA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small search-hit tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3712285.3759866">10.1145/3712285.3759866 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The First Star-by-star <span class="MathJax_Preview">N</span><script type="math/tex">N</script>-body/Hydrodynamics Simulation of Our Galaxy Coupling with a Surrogate Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hirashima%2C+K">Keiya Hirashima</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fujii%2C+M+S">Michiko S. Fujii</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saitoh%2C+T+R">Takayuki R. Saitoh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Harada%2C+N">Naoto Harada</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nomura%2C+K">Kentaro Nomura</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yoshikawa%2C+K">Kohji Yoshikawa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hirai%2C+Y">Yutaka Hirai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Asano%2C+T">Tetsuro Asano</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moriwaki%2C+K">Kana Moriwaki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Iwasawa%2C+M">Masaki Iwasawa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Okamoto%2C+T">Takashi Okamoto</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Makino%2C+J">Junichiro Makino</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2510.23330v1-abstract-short" style="display: inline;">
        …-body/hydrodynamics simulations working with <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>. This approach bypasses the short timesteps caused by supernova explosions using a surrogate model, thereby improving scalability. With this method, we reached 300 billion particles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking through the…
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23330v1-abstract-full').style.display = 'inline'; document.getElementById('2510.23330v1-abstract-short').style.display = 'none';">▽ More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2510.23330v1-abstract-full" style="display: none;">
        A major goal of computational astrophysics is to simulate the Milky Way Galaxy with sufficient resolution down to individual stars. However, the scaling fails due to some small-scale, short-timescale phenomena, such as supernova explosions. We have developed a novel integration scheme of <span class="MathJax_Preview">N</span><script type="math/tex">N</script>-body/hydrodynamics simulations working with <span class="search-hit mathjax">machine</span> <span class="search-hit mathjax">learning</span>. This approach bypasses the short timesteps caused by supernova explosions using a surrogate model, thereby improving scalability. With this method, we reached 300 billion particles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking through the billion-particle barrier currently faced by state-of-the-art simulations. This resolution allows us to perform the first star-by-star galaxy simulation, which resolves individual stars in the Milky Way Galaxy. The performance scales over <span class="MathJax_Preview">10^4</span><script type="math/tex">10^4</script> CPU cores, an upper limit in the current state-of-the-art simulations using both A64FX and X86-64 processors and NVIDIA CUDA GPUs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2510.23330v1-abstract-full').style.display = 'none'; document.getElementById('2510.23330v1-abstract-short').style.display = 'inline';">△ Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 7 figures, 7 tables, IEEE/ACM Supercomputing Conference (SC25)</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          RIKEN-iTHEMS-Report-25
        

        

        
      </p>
    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="" class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50" class="pagination-next">Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0" class="pagination-link is-current" aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50" class="pagination-link " aria-label="Page 2" aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100" class="pagination-link " aria-label="Page 3" aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150" class="pagination-link " aria-label="Page 4" aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Machine+Learning&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200" class="pagination-link " aria-label="Page 5" aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">…</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"></path></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"></path></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  
<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Size1, monospace;"></div></div></body></html>